{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d600d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697384b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds , boxes_real , box_format = \"midpoint\"):\n",
    "    #top left point of the image is 0,0 and buttom right corner is 1,1\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[... , 0:1] - (boxes_preds[... , 2:3]/2)\n",
    "        box1_y1 = boxes_preds[... , 1:2] - (boxes_preds[... , 3:4]/2) \n",
    "        box1_x2 = boxes_preds[... , 0:1] + (boxes_preds[... , 2:3]/2)\n",
    "        box1_y2 = boxes_preds[... , 1:2] + (boxes_preds[... , 3:4]/2)\n",
    "\n",
    "        box2_x1 = boxes_real[... , 0:1] - (boxes_real[... , 2:3]/2)\n",
    "        box2_y1 = boxes_real[... , 1:2] - (boxes_real[... , 3:4]/2)\n",
    "        box2_x2 = boxes_real[... , 0:1] + (boxes_real[... , 2:3]/2)\n",
    "        box2_y2 = boxes_real[... , 1:2] + (boxes_real[... , 3:4]/2)\n",
    "        \n",
    "        \n",
    "    if box_format ==\"edges\":\n",
    "        box1_x1 = boxes_preds[... , 0:1]\n",
    "        box1_y1 = boxes_preds[... , 1:2]\n",
    "        box1_x2 = boxes_preds[... , 2:3]\n",
    "        box1_y2 = boxes_preds[... , 3:4]\n",
    "\n",
    "        box2_x1 = boxes_real[... , 0:1]\n",
    "        box2_y1 = boxes_real[... , 1:2]\n",
    "        box2_x2 = boxes_real[... , 2:3]\n",
    "        box2_y2 = boxes_real[... , 3:4]\n",
    "    \n",
    "    x1 = torch.max(box1_x1 , box2_x1)\n",
    "    y1 = torch.max(box1_y1 , box2_y1)\n",
    "    x2 = torch.min(box1_x2 , box2_x2)\n",
    "    y2 = torch.min(box1_y2 , box2_y2)\n",
    "    \n",
    "    intersection_area = (x2 - x1).clamp(0)  *  (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    union_area = (box1_area + box2_area - intersection_area + 1e-6) #adding a very small value to avoid division by 0\n",
    "    \n",
    "    IOU = intersection_area/union_area\n",
    "    return IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a22e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression (bboxes , iou_thresh , prob_thresh , box_format = \"midpoint\"):\n",
    "  # the shape of the box should be [class , propability of class , x1 , y1 , x2 , y2]\n",
    "\n",
    "  assert type(bboxes) == list  \n",
    "    \n",
    "  bboxes = [box for box in bboxes if box[1] > prob_thresh]\n",
    "  bboxes = sorted(bboxes, key = lambda x: x[1] , reverse = True)\n",
    "  bboxes_after_nms = []\n",
    "\n",
    "  while(bboxes):\n",
    "    chosen_box = bboxes.pop(0)\n",
    "    bboxes = [box for box in bboxes if box[0]!= chosen_box[0] \n",
    "              or intersection_over_union(torch.tensor(chosen_box[2:]) , torch.tensor(box[2:]) , box_format = box_format) < iou_thresh]\n",
    "    \n",
    "    bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "  return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (pred_boxes , true_boxes , iou_thresh = 0.5 , box_format = \"midpoint\" , num_classes = 20):\n",
    "    #pred_boxes = [[index , class_pred , probability , x1 , y1 , x2 , y2], ...]\n",
    "    average_precisions = []\n",
    "    saver_from_div_0 = 1e-6\n",
    "    \n",
    "    for c in range (num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c :\n",
    "                detections.append(detection)\n",
    "                \n",
    "        for truth in true_boxes:\n",
    "            if truth[1] == c :\n",
    "                ground_truths.append(truth)\n",
    "                \n",
    "        #number of real bounding boxes for class c in all the dataset for each image \n",
    "        #returns dectionary {image0: #bboxes for image 0 , ...} \n",
    "        amount_bboxes = Counter([t[0] for t in ground_truths])  \n",
    "        \n",
    "        for key , val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "            \n",
    "        detections.sort(key = lambda x: x[2] , reverse = True)\n",
    "        true_pos  = torch.zeros((len(detections)))\n",
    "        false_pos = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        #if there is no bbox for this image then just continue to the next image\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "        \n",
    "        for detection_idx , detection in enumerate(detections):\n",
    "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0 \n",
    "            \n",
    "            for idx_bbox , bbox in enumerate(ground_truth_img):\n",
    "                new_iou = intersection_over_union(torch.tensor(detection[3:]) , torch.tensor(bbox[3:]) ,  box_format = box_format)\n",
    "                if new_iou > best_iou:\n",
    "                    best_iou = new_iou\n",
    "                    best_iou_idx = idx_bbox\n",
    "                    \n",
    "            if best_iou > iou_thresh:\n",
    "                if amount_bboxes[detection[0]][best_iou_idx] == 0:\n",
    "                    amount_bboxes[detection[0]][best_iou_idx] = 1\n",
    "                    true_pos[detection_idx] = 1\n",
    "\n",
    "                else:\n",
    "                    false_pos[detection_idx] = 1\n",
    "\n",
    "            else:\n",
    "                false_pos[detection_idx] = 1\n",
    "                    \n",
    "            #now we measured the true positives and FP  for all the images in our prediction\n",
    "            #and now we can calculate percision and recall\n",
    "            \n",
    "            tp_cumsum = torch.cumsum(true_pos , dim = 0)\n",
    "            fp_cumsum = torch.cumsum(false_pos , dim = 0)\n",
    "            \n",
    "            recalls = tp_cumsum / (total_true_bboxes + saver_from_div_0)\n",
    "            percisions = torch.divide(tp_cumsum ,(tp_cumsum + fp_cumsum + saver_from_div_0))\n",
    "            \n",
    "            recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "            percisions = torch.cat((torch.tensor([1]), percisions))\n",
    "            \n",
    "            #trapazoide function calculates the area under a curve with y points as first idx \n",
    "            #and x points as 2nd idx\n",
    "            area_under_PR = torch.trapz(percisions , recalls)\n",
    "            \n",
    "            #now we add a mean average percision for class c  \n",
    "            average_precisions.append(area_under_PR)\n",
    "    \n",
    "    #now get the average of all class averages\n",
    "    MAP = sum(average_precisions) / (len(average_precisions) + saver_from_div_0)\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c53b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def plot_image(image, boxes, labels=None):\n",
    "    \"\"\"Plots predicted bounding boxes on the image with optional labels\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle patch\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add label if available\n",
    "        if labels:\n",
    "            plt.text(\n",
    "                upper_left_x * width,\n",
    "                upper_left_y * height,\n",
    "                f\"{labels}\",\n",
    "                color=\"r\",\n",
    "                backgroundcolor=\"white\",\n",
    "                fontsize=8,\n",
    "            )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28927af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% when threading works\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes =  non_max_supression(\n",
    "                bboxes[idx],\n",
    "                iou_thresh=iou_threshold,\n",
    "                prob_thresh=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7 , b=2 , c=20 , device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "    #%%%%%%%%%%%%%%%% When we can use cuda\n",
    "    predictions = predictions.to(device)\n",
    "    batch_size = predictions.shape[0]  #num images you made prediction for\n",
    "    predictions = predictions.reshape(batch_size, S, S, (c+5*b))\n",
    "    bboxes1 = predictions[..., 21:25]\n",
    "    bboxes2 = predictions[..., 26:30]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(S).repeat(batch_size, S , 1).unsqueeze(-1).to(device)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(-1)\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim = -1)\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    #converts the bboxes outputed from convert_cellboxes to normal lists insted of \n",
    "    #pytorch tensors to be able to put them in a file again or compare them to the \n",
    "    #outputs in the files\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ee889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fe50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    #(kernel size , #filters , stride , padding)\n",
    "    (7,64,2,3),\n",
    "    #max pooling layer (2,2)stride = 2\n",
    "    \"m\",\n",
    "    (3,192,1,1),\n",
    "    \n",
    "    #m (2,2)\n",
    "    \"m\",\n",
    "    \n",
    "    (1,128,1,0),\n",
    "    (3,256,1,1),\n",
    "    (1,256,1,0),\n",
    "    (3,512,1,1),\n",
    "    \n",
    "    #m(2,2) \n",
    "    \"m\",\n",
    "    \n",
    "    #these two conv layers repeated 4 times\n",
    "    [(1,256,1,0), (3,512,1,1), 4],\n",
    "\n",
    "    #***\n",
    "    (1,512,1,0),\n",
    "    (3,1024,1,1),\n",
    "    \n",
    "    #m (2,2)\n",
    "    \"m\",\n",
    "    [(1,512,1,0) , (3,1024,1,1),2],\n",
    "\n",
    "    #***\n",
    "    (3,1024,1,1),\n",
    "    (3,1024,2,1),\n",
    "    (3,1024,1,1),\n",
    "    (3,1024,1,1)   \n",
    "    #final output os of size 7*7*1024\n",
    "    #total 24 conv layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_block(nn.Module):\n",
    "    #kwargs is for specifying the stride , kernel size , padding\n",
    "    def __init__(self , in_channels , out_channels , **kwargs):\n",
    "        super(cnn_block , self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels , out_channels , bias = False , **kwargs)\n",
    "        #make batch normalization so that training is speed up\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        #apply the leaky relu activation function to the output:\n",
    "        #0.1 is the slope we will be multiplying to the negative part of the prediction \n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward (self , x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolov1(nn.Module):\n",
    "    def __init__ (self , in_channels = 3 , **kwargs):\n",
    "        super(yolov1 , self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        #here we create the convolutional layers based on the architecture we specified\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        \n",
    "        #here we create a fully connected layer\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x , start_dim = 1) # make the vector of each kernel seperate in 1 dimention it will then be totaly flattened indide _create_fcs method\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "     \n",
    "    #make this mehod private\n",
    "    def _create_conv_layers (self  , architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    cnn_block(\n",
    "                    in_channels , out_channels = x[1] , kernel_size = x[0] , stride = x[2] , padding = x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "                \n",
    "            if type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size = (2, 2) , stride = (2, 2))]\n",
    "                \n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "                \n",
    "                for i in range (num_repeats):\n",
    "                    layers += [\n",
    "                    cnn_block(\n",
    "                    in_channels , out_channels = conv1[1] , kernel_size = conv1[0] , stride = conv1[2] , padding = conv1[3],\n",
    "                    )\n",
    "                    ]\n",
    "                    \n",
    "                    layers += [\n",
    "                    cnn_block(\n",
    "                    conv1[1] , out_channels = conv2[1] , kernel_size = conv2[0] , stride = conv2[2] , padding = conv2[3],\n",
    "                    )\n",
    "                    ]              \n",
    "                    in_channels = conv2[1]\n",
    "       \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def _create_fcs (self , split_size , num_boxes , num_classes):\n",
    "        s , b , c = split_size , num_boxes , num_classes\n",
    "        return nn.Sequential(\n",
    "            #flatten layer of the 7*7*1024 out of the fature map\n",
    "            nn.Flatten(),\n",
    "            #first fully connected layer that is 496 in size __ originallly in hte paper it was with 4096\n",
    "            #$$$$$$$$$$$$$$$$$$$$$$$$444we made this  instead of 4096 in paper for power limitations\n",
    "            nn.Linear(1024 * s * s , 496), #it takes input size and output size\n",
    "            #drop out layer for normalization\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            #$$$$$$$$$$$$$$$$$$$$$$$$444we made this  instead of 4096 in paper for power limitations\n",
    "            nn.Linear(496 , s*s * (c+(b*5))), #the output of this is still 1d and we will have to reshape it          \n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "def test(s = 7 , b = 2 , c = 20):\n",
    "        model = yolov1(split_size = s , num_boxes = b , num_classes = c)\n",
    "        x = torch.randn((2,3,448,448))\n",
    "        #call the forward function\n",
    "        print(model(x).shape)\n",
    "    \n",
    "test()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolo_loss(nn.Module):\n",
    "    def __init__(self , s = 7 , b = 2 , c = 20):\n",
    "        super(yolo_loss,self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction = \"sum\")\n",
    "        self.s = s\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "        \n",
    "    def forward(self , predictions , targets):\n",
    "        #predictions for 1 bbox at dimention 3: [p0 , p0 ,..., p19 , c1 , x1 , y1 ,w1 , h1 , x2 , y2 ,w2 , h2]\n",
    "        # target shape -> [p0 , p0 ,..., p19 , c1 , x1 , y1 ,w1 , h1 , 0 , 0 , 0 ,0]\n",
    "        predictions = predictions.reshape(-1 , self.s , self.s , (self.c + (self.b * 5)))\n",
    "        iou_b1 = intersection_over_union(predictions[...,21:25] , targets[...,21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[...,26:30] , targets[...,21:25])\n",
    "        \n",
    "        ious = torch.cat([iou_b1.unsqueeze(0) , iou_b2.unsqueeze(0)] , dim = 0)\n",
    "        iou_maxes , best_box = torch.max(ious , dim = 0)\n",
    "        \n",
    "        #identity matrix for obj matrix\n",
    "        exists_box = targets[...,20].unsqueeze(3)  #we could have make it exists_box = target[...,20:21]\n",
    "        \n",
    "        ###************ calculate error for bbox coordinates**********************\n",
    "        boxes_predictions = exists_box * (\n",
    "            (\n",
    "               best_box * predictions [... , 26:30] +\n",
    "               (1- best_box) * predictions [... , 21:25]\n",
    "            )\n",
    "        )\n",
    "        boxes_targets = exists_box * targets [... , 21:25]\n",
    "        #now we want instead of having w , h we want sqrt of w , sqrt(h) to exist\n",
    "        #add 1e-6 to the sqrt because when we calculate the derivative if sqrt = 0\n",
    "        #the der = infinity\n",
    "        \n",
    "        boxes_predictions [... , 2:4] = torch.sign(boxes_predictions[... , 2:4]) * torch.sqrt(\n",
    "                torch.abs(boxes_predictions[... , 2:4]+ 1e-6) \n",
    "            )\n",
    "        \n",
    "        # *****************why when i added an epsilon here it reached to some map and stopped at this value\n",
    "        boxes_targets [..., 2:4] = torch.sqrt(boxes_targets[... , 2:4])\n",
    "        \n",
    "        boxes_losses = self.mse(\n",
    "            torch.flatten(boxes_predictions , end_dim = -2) ,\n",
    "            torch.flatten(boxes_targets , end_dim = -2)\n",
    "        )      \n",
    "        \n",
    "        \n",
    "        ###************ calculate error for negative errors (obj case) ***************************\n",
    "        #(N,s,s,1) -> (N*S*S)\n",
    "        boxes_pre_obj = exists_box*(best_box * predictions[...,25:26] +\n",
    "                                        (1-best_box) * predictions[...,20:21])\n",
    "        boxes_tar_obj = exists_box*(targets[...,20:21])\n",
    "        \n",
    "        boxes_obj_loss = self.mse(\n",
    "           torch.flatten(boxes_pre_obj),  #%%%%%%%%%%%%%%%%%%%%%%% why not end_dim = -2 %%%%%%%%%%%%%%%%%%%%\n",
    "           torch.flatten(boxes_tar_obj)\n",
    "        )\n",
    "         \n",
    "        \n",
    "        ###************ calculate error for positive errors (no obj case)\n",
    "        #(N,s,s,1) -> (N, S*S)\n",
    "        boxes1_pre_noobj = (1 - exists_box) * (predictions[...,20:21])\n",
    "        boxes2_pre_noobj = (1 - exists_box) * (predictions[...,25:26])                                      \n",
    "        boxes_tar_noobj =  (1 - exists_box) * (targets[...,20:21])\n",
    "        \n",
    "        boxes_noobj_loss = self.mse(\n",
    "           torch.flatten(boxes1_pre_noobj , start_dim = 1),  #%%%%%%%%%%%%%%%%%%%%%%% why not end_dim = -2 %%%%%%%%%%%%%%%%%%%%\n",
    "           torch.flatten(boxes_tar_noobj  , start_dim = 1)\n",
    "        )\n",
    "                                               \n",
    "        boxes_noobj_loss += self.mse(\n",
    "           torch.flatten(boxes2_pre_noobj , start_dim = 1),  #%%%%%%%%%%%%%%%%%%%%%%% why not end_dim = -2 %%%%%%%%%%%%%%%%%%%%\n",
    "           torch.flatten(boxes_tar_noobj , start_dim = 1)\n",
    "        )\n",
    "        \n",
    "        ###************ calculate error for class errors\n",
    "        class_predictions = exists_box * predictions[... , :20]\n",
    "        class_targets = exists_box * targets[... , :20]\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(class_predictions , end_dim = -2 , ),\n",
    "            torch.flatten(class_targets , end_dim = -2)\n",
    "        )  \n",
    "        \n",
    "        #total loss *************************************************\n",
    "        total_loss = (self.lambda_coord * boxes_losses +\n",
    "                                               boxes_obj_loss +\n",
    "                                               self.lambda_noobj * boxes_noobj_loss +\n",
    "                                               class_loss\n",
    "                     )                        \n",
    "        return total_loss                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pascal_dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self , csv_file , img_dir , label_dir , s = 7 ,b = 2, c = 20 ,transform = None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.s = s\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        #returns num of samples used in the dataset\n",
    "        return  len(self.annotations)\n",
    "    \n",
    "    def __getitem__ (self , index):\n",
    "        label_path = os.path.join(self.label_dir , self.annotations.iloc[index , 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label , x , y , width , height = [\n",
    "                    float(x) if float(x)!=int(float(x)) else int (x) \n",
    "                    for x in label.replace(\"\\n\" , \"\").split()\n",
    "                ] \n",
    "                boxes.append([class_label , x , y , width , height])\n",
    "\n",
    "\n",
    "        img_path = os.path.join(self.img_dir ,self.annotations.iloc[index ,0])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        boxes = torch.tensor(boxes)  \n",
    "        if self.transform:\n",
    "            image , bboxes = self.transform(image , boxes)\n",
    "\n",
    "        label_matrix = torch.zeros((self.s , self.s , self.c + self.b * 5))\n",
    "        for box in boxes:\n",
    "            class_label , x , y , width , height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "            #get i,j of the middle point\n",
    "            i,j = int(self.s * y) , int(self.s * x)\n",
    "            #get x , y coord relative to the cell responsiple for the prediction\n",
    "            x_cell , y_cell = ((self.s * x) - j)  , ((self.s * y) - i)\n",
    "\n",
    "            width_cell , height_cell =  (width * self.s ,\n",
    "                                         height * self.s)\n",
    "\n",
    "            if label_matrix[i , j , 20] == 0:\n",
    "                label_matrix[i , j , 20] = 1\n",
    "                box_coordinates = torch.tensor([x_cell , y_cell , width_cell , height_cell])\n",
    "                label_matrix[i , j , 21:25] = box_coordinates\n",
    "                label_matrix[i , j , class_label] = 1\n",
    "\n",
    "\n",
    "        return image , label_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "learning_rate = 2e-5\n",
    "batch_size = 12\n",
    "#for sake of simplicity we can let weight decay regularization= 0    \n",
    "weight_decay = 0\n",
    "epochs = 600\n",
    "#num workers who will be loading the data i.e. number of threads\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5 num workers = 2 if wanted threading\n",
    "num_workers = 0\n",
    "#we will try to access the GPU for loading the data\n",
    "pin_memory = True\n",
    "load_model = False\n",
    "device = \"cpu\"\n",
    "model_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\yolo_model.pth.tar\"  #in shaa allah will put here the directory of model after training \n",
    "\n",
    "img_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\images\"\n",
    "labels_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\labels\"\n",
    "\n",
    "class compose(object):\n",
    "    def __init__ (self , transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__ (self , img , bboxes):\n",
    "        for t in self.transforms:\n",
    "            img , bboxes = t(img) , bboxes\n",
    "          \n",
    "        return img , bboxes\n",
    "        \n",
    "\n",
    "transform = compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n",
    "\n",
    "def train_fn (train_loader , model , optimizer , loss_fn):\n",
    "    loop = tqdm(train_loader , leave = True)   \n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx , (x ,y) in enumerate(loop):\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% when threading works\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn (out , y)\n",
    "        mean_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad() # make the optimization is when we reach a zero slope dl / dw\n",
    "        loss.backward()  # make backward propagation calculate dl / dw\n",
    "        optimizer.step()  # update parameters\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "    \n",
    "    print (f\"Mean loss is {sum(mean_loss) / (len(mean_loss)+ 1e-6)}\")\n",
    "        \n",
    "def main ():\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% when threading works\n",
    "    model = yolov1 (split_size = 7 , num_boxes = 2 , num_classes = 20).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters() , lr = learning_rate , weight_decay = weight_decay\n",
    "    )\n",
    "    loss_fn = yolo_loss()\n",
    "    \n",
    "    if load_model:\n",
    "        load_checkpoint(torch.load(model_dir), model, optimizer)\n",
    "    \n",
    "    train_dataset = pascal_dataset(\n",
    "        \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\100examples.csv\",\n",
    "        transform = transform,\n",
    "        img_dir = img_dir,\n",
    "        label_dir = labels_dir\n",
    "              \n",
    "    )\n",
    "    \n",
    "    test_dataset = pascal_dataset(\n",
    "        \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\test.csv\",\n",
    "        transform = transform, \n",
    "        img_dir = img_dir,\n",
    "        label_dir = labels_dir\n",
    "             \n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory,\n",
    "        shuffle = True ,\n",
    "        drop_last = False,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset = test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory,\n",
    "        shuffle = True ,\n",
    "        drop_last = True,\n",
    "    )\n",
    "    \n",
    "    max_map = -1\n",
    "    for epoch in range (epochs):                  \n",
    "        pred_boxes , target_boxes = get_bboxes(train_loader , model , iou_threshold = 0.5 , threshold = 0.4)\n",
    "        mean_avg_percision = performance(pred_boxes , target_boxes , iou_thresh = 0.5 , box_format = \"midpoint\")\n",
    "        print(f\"mean average percision: {mean_avg_percision}\")\n",
    "        print(\"epoch: \" , epoch)\n",
    "         \n",
    "        ##save a checkpoint of the model each time we reach a good MAP\n",
    "        if mean_avg_percision > 0.9:\n",
    "            if mean_avg_percision > 0.99 and epoch >=200:\n",
    "                checkpoint = {\n",
    "                   \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "                save_checkpoint(checkpoint, filename=model_dir)\n",
    "                time.sleep(10)\n",
    "                break\n",
    "                \n",
    "            elif mean_avg_percision > max_map:\n",
    "                max_map = mean_avg_percision\n",
    "                checkpoint = {\n",
    "                   \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "                save_checkpoint(checkpoint, filename=model_dir)\n",
    "                time.sleep(10)\n",
    "\n",
    "        train_fn (train_loader , model , optimizer , loss_fn)   \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "learning_rate = 2e-5\n",
    "batch_size = 12\n",
    "#for sake of simplicity we can let weight decay regularization= 0    \n",
    "weight_decay = 0\n",
    "epochs = 600\n",
    "#num workers who will be loading the data i.e. number of threads\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5 num workers = 2 if wanted threading\n",
    "num_workers = 0\n",
    "#we will try to access the GPU for loading the data\n",
    "pin_memory = True\n",
    "load_model = False\n",
    "device = \"cpu\"\n",
    "model_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\yolo_model.pth.tar\"  #in shaa allah will put here the directory of model after training \n",
    "\n",
    "img_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\images\"\n",
    "labels_dir = \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\labels\"\n",
    "\n",
    "class compose(object):\n",
    "    def __init__ (self , transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__ (self , img , bboxes):\n",
    "        for t in self.transforms:\n",
    "            img , bboxes = t(img) , bboxes\n",
    "          \n",
    "        return img , bboxes\n",
    "        \n",
    "\n",
    "transform = compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "def test_yolo (model_dir  , test_data , s=7 , b=2 ,c=20 ):\n",
    "    \n",
    "    model = yolov1 (split_size = s , num_boxes = b , num_classes = c).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters() , lr = learning_rate , weight_decay = weight_decay\n",
    "    )\n",
    "    load_checkpoint(torch.load(model_dir), model, optimizer)\n",
    "    test_dataset = pascal_dataset(\n",
    "        test_data,\n",
    "        transform = transform, \n",
    "        img_dir = img_dir,\n",
    "        label_dir = labels_dir\n",
    "             \n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset = test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory,\n",
    "        shuffle = True ,\n",
    "        drop_last = True,\n",
    "    )\n",
    "    #################### plot the images\n",
    "    for x,y in (test_loader):\n",
    "        for idx in range(8):\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            out = cellboxes_to_boxes(out)\n",
    "            out = non_max_supression(out[idx], iou_thresh=0.5, prob_thresh=0.4, box_format=\"midpoint\")\n",
    "            ##to plot the image we need to change its dimentions from(channels , height , width) -> (hight,width ,channels)\n",
    "            classd = out[0][0]\n",
    "            \n",
    "            if (classd == 0):\n",
    "                pred = [\"aeroplane\"]\n",
    "            elif (classd == 1):\n",
    "                pred = [\"bicycle\"]\n",
    "            elif (classd == 2):\n",
    "                pred = [\"bird\"]\n",
    "            elif (classd == 3):\n",
    "                pred = [\"boat\"]\n",
    "            elif (classd == 4):\n",
    "                pred = [\"bottle\"]\n",
    "            elif (classd == 5):\n",
    "                pred = [\"bus\"]\n",
    "            elif (classd == 6):\n",
    "                pred = [\"car\"]\n",
    "            elif (classd == 7):\n",
    "                pred = [\"cat\"]\n",
    "            elif (classd == 8):\n",
    "                pred = [\"chair\"]\n",
    "            elif (classd == 9):\n",
    "                pred = [\"cow\"]\n",
    "            elif (classd == 10):\n",
    "                pred = [\"dining table\"]\n",
    "            elif (classd == 11):\n",
    "                pred = [\"dog\"]\n",
    "            elif (classd == 12):\n",
    "                pred = [\"horse\"]\n",
    "            elif (classd == 13):\n",
    "                pred = [\"motorbike\"]\n",
    "            elif (classd == 14):\n",
    "                pred = [\"person\"]\n",
    "            elif (classd == 15):\n",
    "                pred = [\"potted plant\"]\n",
    "            elif (classd == 16):\n",
    "                pred = [\"sheep\"]\n",
    "            elif (classd == 17):\n",
    "                pred = [\"sofa\"]\n",
    "            elif (classd == 18):\n",
    "                pred = [\"train\"]\n",
    "            elif (classd == 19):\n",
    "                pred = [\"TV monitor\"]\n",
    "            else:\n",
    "                pred = [\"Unknown\"]\n",
    "\n",
    "                 \n",
    "            \n",
    "            plot_image(x[idx].permute(1,2,0).to(\"cpu\"), out , pred)\n",
    "    ######################## plot the images \n",
    "    \n",
    "test_yolo(model_dir , \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a395b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = yolov1 (split_size = 7 , num_boxes = 2 , num_classes = 20).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters() , lr = learning_rate , weight_decay = weight_decay\n",
    ")\n",
    "load_checkpoint(torch.load(model_dir), model, optimizer)\n",
    "test_dataset = pascal_dataset(\n",
    "    \"D:\\\\هنا حيث الروعة كلها\\\\machine learning\\\\archive (6)\\\\test.csv\",\n",
    "    transform = transform, \n",
    "    img_dir = img_dir,\n",
    "    label_dir = labels_dir\n",
    "\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = pin_memory,\n",
    "    shuffle = True ,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in (test_loader):\n",
    "        x = x.to(device)\n",
    "        y_eval = model.forward(x)\n",
    "        loss_fn = yolo_loss()\n",
    "        loss = loss_fn(y_eval , y)\n",
    "        \n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
